[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPageRank\n\n\n\nData Science\n\n\nLinear Algebra\n\n\nMacalester\n\n\n\nImplementing and Exploring a Simplified Version of the PageRank Algorithm. This paper was created as part of a mid-semester project for Math/Comp 365: Computational Linear…\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDoubly Robust Estimators\n\n\n\nStatistics\n\n\nMacalester\n\n\n\nAn Exploration of a Double Robust Estimator for Causal Inference. This paper was created as a final project for Stat 451: Causal Inference, taught by Leslie Myint.\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDefining Achievement\n\n\n\nData Science\n\n\nEducation\n\n\nMacalester\n\n\n\nInvestigating the Factors that Define Educational Achievement using California as a Case Study. This site was created as a final project for Stat/Comp 456: Projects in Data…\n\n\n\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsteroid Value Analysis\n\n\n\nData Science\n\n\nSpace\n\n\nMacalester\n\n\n\nVisualization and Analysis of Asterank data. This paper was created as part of a final project for Stat/Comp 112: Introduction to Data Science taught by Brianna Heggeseth.\n\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAdvection Diffusion with Simple Erosion and Deposition\n\n\n\nMath\n\n\nMacalester\n\n\n\nA simple mathematical model of nitrogen runoff. This paper was created as part of final project for Math 432: Mathematical Modeling, taught by Will Mitchell.\n\n\n\nNathaniel Reimer\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA Time Series Approach to Modeling Potential Solar Output\n\n\n\nData Science\n\n\nSun\n\n\nMacalester\n\n\n\nWe use time-series methods to model Global Horizontal Irradiance (GHI) at the hourly and daily levels. This paper was created as a final project for Stat 452: Correlated…\n\n\n\nNathaniel Reimer, Kyle Suelflow\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/pagerank/index.html",
    "href": "projects/pagerank/index.html",
    "title": "PageRank",
    "section": "",
    "text": "PageRank with Eigenvectors cla365 technical report\n\n\n\n \n\n\n\n\n\nPageRank with Eigenvectors cla365 technical report \n\n\n Nathaniel Reimer     Preston Locke \n\n\n(April 30, 2022)\n\n\n\nAbstract\n\n\nIn this report we examine the PageRank algorithm by showing that the Google Matrix is stochastic. We then present R code that implements the PageRank algorithm and demonstrate it on a simple network. As an extension, we explore some permutations on this network as well as the implications of changing the random jump probability (q)\n\n\n\n\n1 Implementing PageRank\n\n\n\n1.1 Proving the Google Matrix is Stochastic\n\n\n\nProving the Google matrix G is stochastic requires showing that G is nonnegative and that each of its columns sum to 1. We’ll start by proving that the matrix is nonnegative. Note that for each entry in G, we have the following:\n\n\n\n\n\n\n\n\nGi⁢j=q/n+Aj⁢i⁢(1-q)/nj\n\n\n\n\n\n\n\n\nWe already know the following facts about the variables in the expression above:\n\n\n\n\n\n1.\n\n\n0≤q≤1\n\n\n\n\n2.\n\n\nn&gt;0\n\n\n\n\n3.\n\n\nAj⁢i∈{0,1}\n\n\n\n\n4.\n\n\nnj=∑i=1nAj⁢i\n\n\n\n\n\n\n\nFrom facts 1, 2, and 3, we also know that q/n, Aj⁢i, and (1-q) are all nonnegative numbers. And then, by facts 3 and 4, nj must also be nonnegative. Therefore, we can say that the entire expression is nonnegative, and thus, each entry in G is nonnegative by definition.\n\n\n\n\nNext, we show that each column in G sums to 1, or in other words, that for each j∈{1,2,…,n} we show that:\n\n\n\n\n\n\n\n\n∑i=1nq/n+Aj⁢i⁢(1-q)/nj=1\n\n\n\n\n\n\n\n\nThe first term in the summation can be pulled out because it’s easy to see that q/n added n times will just equal q. We can also pull out the expression ((1-q)/nj) from the summation, because everything in it is a constant with respect to i. Doing these two things gives us the below:\n\n\n\n\n\n\n\n\nq+((1-q)/nj)⁢∑i=1nAj⁢i\n\n\n\n\n\n\n\n\nNow, using fact 4 from the first part of our proof, we can simplify ∑i=1nAj⁢i to nj and finish simplifying:\n\n\n\n\n\n\n\n\nq+((1-q)/nj)⁢nj\n\n\n\n\n\n\n\n\n\n\nq+(1-q)\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\nWith that, we have successfully shown that G is a nonnegative matrix whose columns sum to 1, so we have proven that G is stochastic.\n\n\n\n\n\n1.2 Calculating the Google Matrix and Page Ranks\n\n\n\nTo calculate page ranks for a given network of pages, we can use the fact that G is stochastic and find its dominant eigenvector, which will give us the rankings. If we scale that eigenvector so that its entries sum to 1, we’ll have our answer. To do this, we’ve written two R functions, GoogleMatrix and PageRankVector, which take in an n×n adjacency matrix adj and output, respectively, the n×n Google matrix of the network and a vector of page ranks given the value of q (which defaults to 0.15).\n\n\n\nGoogleMatrix = function(adj, q=0.15) {\n  n = dim(adj)[1]\n  G = matrix(nrow=n, ncol=n)\n  for (j in 1:n) {\n    # Get the sum of row j\n    n.j = sum(adj[j,])\n    for (i in 1:n) {\n      # Calculate the ijth entry of G\n      G[i,j] = q/n + adj[j,i] * (1 - q) / n.j\n    }\n  }\n  return(G)\n}\nPageRankVector = function(adj, q=0.15) {\n  G = GoogleMatrix(adj, q=q)\n  # Find the dominant eigenvector of G\n  p = eigen(G)$vectors[,1]\n  # Return the normalized version (inf. norm)\n  return(p / sum(p))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Network of 15 pages, taken from page 550 of the textbook.\n\n\n\n\nWe can test these functions on the network in Figure 1 that is taken from the textbook. The result of running GoogleMatrix is shown in Appendix A, but the result of running PageRankVector is shown below:\n\n\n\n&gt; A = rbind(\n+     c(0,1,0,0,0,0,0,0,1,0,0,0,0,0,0),\n+     c(0,0,1,0,1,0,1,0,0,0,0,0,0,0,0),\n+     c(0,1,0,0,0,1,0,1,0,0,0,0,0,0,0),\n+     c(0,0,1,0,0,0,0,0,0,0,0,1,0,0,0),\n+     c(1,0,0,0,0,0,0,0,0,1,0,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,0,0,0),\n+     c(0,0,0,1,0,0,0,0,0,0,1,0,0,0,0),\n+     c(0,0,0,0,1,1,0,0,0,1,0,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0),\n+     c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1),\n+     c(0,0,0,0,0,0,1,1,0,0,1,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,1,0,0,0,0,1,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,1,0,1),\n+     c(0,0,0,0,0,0,0,0,0,0,0,1,0,1,0)\n+ )\n&gt; PageRankVector(A)\n [1] 0.027 0.030 0.030 0.027 0.040 0.040 0.040 0.040 0.075 0.106 0.106 0.075 0.125 0.116 0.125\n\n\n\n\nHigher numbers in the PageRank vector correspond to higher rankings. So, using the PageRank method with q=.15 makes 13 and 15 the highest ranked websites and 1 and 4 the lowest ranked.\n\n\n\n\n\n\n2 Further Exploration\n\n\n\n2.1 Other Values of q\n\n\n\nThe jump probability, q, is the probability that a web-surfer chooses any random website rather than clicking one of the links on their current website. We can illustrate this using the equation for the entries to the Google matrix G.\n\n\n\n\n\n\nGi⁢j=qn+Ai⁢j⁢(1-q)/nj\n\n\n\n\n\n\nIf q=1 then Gi⁢j=qn since the other term is multiplied by 0. As we expect, the probability of ending up at any particular website is just 1/n where n is the number of websites: after visiting a website a web-surfer simply chooses another website at random from the list. We use the image() function in R to visualize the page rank vector for different values of q. In Figure 2, higher values in the PageRank vector are in yellow and lower values are in dark blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Heatmap of PageRank vectors at different q values.\n\n\n\n\nThe result we obtain from Figure 2 is precisely what we expect. At a q value of 1 all of the websites’ entries in the PageRank vector are exactly equal (they are the same color). Moreover, as we decrease q differences appear and become more pronounced. When q is zero and the probability of ending up at any given website is based solely on the network structure: There are no random jumps. For PageRank, Google used a q value of .15 so that network structure is the primary determining factor but some random jumping occurs. This choice has a number of interesting implications for our network. We first observed that while 15, 14, and 13 have similar values when q is zero, when q reaches .15 website 14 has fallen further than 13 and 15. Additionally, 2 and 3 have lower values when q is zero but higher values when q is .15. It would be unrealistic to assume there is no random jumping when someone is surfing the web and not accounting for it leads to an incorrect page rank, especially in the case of 1, 2, 3, and 4. Why, exactly, these changes happen in this network is certainly worthy of further study. It may come about because of the symmetric connections between 2 and 3 and those between 13, 14, and 15.\n\n\n\n\n\n2.2 Removing a Node\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Network with Page 10 removed.\n\n\n\n\nWhat happens to the ranks of the other pages if we remove a page from the network? For a start, we can predict that the effect on the other page ranks will be more pronounced when a page with a higher rank is removed. Being a probability distribution, the page rank vector needs to add to 1, and thus the value of the removed page’s rank will need to be redistributed to the other pages somehow, in order to maintain the sum of 1. The higher the removed page’s rank, the more value will be redistributed to the other pages.\n\n\n\n\nExploring this theme of removing highly-ranked pages further, let’s look at a concrete example using the 15-page example from before. This time, we’ll remove page 10 to get the network shown in Figure 3. Below are the page ranks we calculated for the modified network:\n\n\n\n&gt; PageRankVector(A[c(1:9, 11:15), c(1:9, 11:15)])\n [1] 0.047 0.041 0.036 0.032 0.043 0.041 0.052 0.050 0.048 0.171 0.104 0.041 0.107 0.186\n\n\n\n\nComparing these rankings to those of the 15-page network, we see that almost all of the pages rose in rank, except for pages 9, 13, and 14. Note that page 13 was pointed to by page 10, and page 13 points to pages 9 and 14. It seems that the popularity of page 10 contributed to propping up the rankings of pages 9, 13, and 14, so when it was removed, those page rankings fell.\n\n\n\n\nIt should be noted, however, that in the given network, there is a directed path from page 10 to every other page in the network, meaning that to some degree, all pages are pointed to by page 10, implying that page 10’s popularity should have had some effect of propping those pages up, if only small. We believe the reason why the rankings of the other pages did not fall is a combination of their degree of separation from page 10 (longer paths yield a smaller influence on ranking) and other factors (like a change in a page’s ratio of in-links to out-links).\n\n\n\n\n\n2.3 Rank Strategy\n\n\n\nWe then considered a possible strategy for a website to increase its PageRank. Specifically, what happens when websites 2 and 12 favor linking to website 7. We model this by changing the corresponding entries in the adjacency matrix from 1 to 2. In Figure 4 we show this change by bolding these two links. We then calculated the PageRank vector p2 for the modified matrix and calculated a ratio of it and the original PageRank vector p1 to obtain a vector p2p1.\n\n\n\n\n\n\n\n\n(p2/p1)T=\n\n\n\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]  [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] 0.969 0.954 0.878 0.892 0.951 0.986 1.33 0.829 1.02  1.05 0.971  0.97  1.04  1.01 0.981\n\n\n\n\n\nFigure 4: Modified network with more prominent links to website 7.\n\n\n\n\nUnsurprisingly, strategy has benefited page 7’s value in the PageRank vector. We decided to take a closer look at how the strategy impacted the values of other sites. We immediately noticed that while 11 and 10 are both linked to by page 7, only page 10’s value increased. This can be explained by examining the network structure. In the google matrix for the modified network the probability of moving from 12 to 7 is increased, this means that the probability of moving from 12 to 11 or 4 is decreased. So even though 11 may benefit from being linked to by 7 this is out weighed by the negative effect of being deprioritized by 12. 10 does not face this negative effect and thus benefits.\n\n\n\n\n\n\n\n\nAppendix A Appendix: Unmodified 15-Page Google Matrix\n\n\n&gt; GoogleMatrix(A)\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n [1,] 0.01 0.01 0.01 0.01 0.43 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [2,] 0.43 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [3,] 0.01 0.29 0.01 0.43 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [4,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.43 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [5,] 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01 0.29  0.01  0.01  0.01  0.01  0.01  0.01\n [6,] 0.01 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.29  0.01  0.01  0.01  0.01  0.01  0.01\n [7,] 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.29  0.01  0.01  0.01\n [8,] 0.01 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.29  0.01  0.01  0.01\n [9,] 0.43 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.43  0.01  0.01\n[10,] 0.01 0.01 0.01 0.01 0.43 0.43 0.43 0.01 0.29  0.01  0.01  0.01  0.01  0.22  0.01\n[11,] 0.01 0.01 0.01 0.01 0.01 0.43 0.43 0.43 0.01  0.01  0.01  0.29  0.01  0.22  0.01\n[12,] 0.01 0.01 0.01 0.43 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.43\n[13,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.86  0.01  0.01  0.01  0.22  0.01\n[14,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.43  0.01  0.43\n[15,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.86  0.01  0.01  0.22  0.01\n\n\n\n\n\n\n\nGenerated by LaTeXML"
  },
  {
    "objectID": "projects/correlated/index.html",
    "href": "projects/correlated/index.html",
    "title": "A Time Series Approach to Modeling Potential Solar Output",
    "section": "",
    "text": "Greenhouse gas emissions have exploded in recent decades, causing substantial damage to the livelihoods of many across the world. One of the main culprits of these emissions is the energy sector. Using non-renewables such as Coal, Oil, and natural gas to create electricity causes carbon and other GHGs to be emitted. To combat this, renewable energy has been brought forward as the leading solution to this crisis. Solar and wind energy, specifically, have the most potential to create lasting change in regards to our energy usage. The reality of this situation has brought us to think about modeling different solar variables. Being able to model Global Horizontal Irradiance, for example, allows places around the globe to gain a better understanding of whether or not they would be suitable for solar energy. Qualitative measures such as economic feasibility, the physical geography of places, and political and regulatory hurdles are other things that places can take into account in order to determine if solar is feasible for them, in addition to considering this analysis (Crook et al. 2011; Wild, Folini, and Henschel 2017; Boland 2020; Shahsavari and Akbari 2018). This leads us into our two research questions:\n\nHow well can we model Global Horizontal Irradiance using a time series approach for different situations across the world? And,\nAre the models we create consistent across the 25 year period (1998-2023) for which we have data from?\n\n\n\nThis project focuses on Global Horizontal Irradiance (GHI), a measure of the power of the power of sunlight that reaches the horizontal ground (Stein, Hansen, and Reno 2012). GHI can then be used to estimate the output of a hypothetical solar installation placed on the ground. GHI data used in this report is part of a publicly available GHI data set from SolarAnywhere. SolarAnywhere uses data from geosynchronous satellites to estimate GHI rather than measuring it directly with ground stations (“Historical Data Validation” 2023). Their model is calibrated with data from ground stations. Annual data is accurate within \\(\\pm\\) 4.2% based on this validation. Errors increase for shorter time periods: monthly data has an RMSE of 3.9% and hourly data has an RMSE of 20.6%.\nWe focus our analysis on Victoria, Seychelles, with comparisons to Nairobi, Kenya, and Manchester, England. Nairobi and Victoria are at similar latitudes and so receive a similar amount of sun throughout the year but have different climates. Manchester’s sunlight is more seasonal because of its latitude and it has a very different climate.\nWe use daily Solar Insolation data in our analysis of daily GHI. Solar Insolation in our data is the amount of downward solar energy incident at the top of the atmosphere (Hartmann 2016). It varies by latitude and time of year. We generate daily insolation data for each of our selected locations using the Python package climlab (Rose et al. 2022). The insolation data has units of \\(\\frac{W}{m^2}\\) per hour. Hourly GHI data has also has units of \\(\\frac{W}{m^2}\\) per hour. The difference is that GHI is measured on the ground and solar insolation is measured at the top of the atmosphere."
  },
  {
    "objectID": "projects/correlated/index.html#introduction",
    "href": "projects/correlated/index.html#introduction",
    "title": "A Time Series Approach to Modeling Potential Solar Output",
    "section": "",
    "text": "Greenhouse gas emissions have exploded in recent decades, causing substantial damage to the livelihoods of many across the world. One of the main culprits of these emissions is the energy sector. Using non-renewables such as Coal, Oil, and natural gas to create electricity causes carbon and other GHGs to be emitted. To combat this, renewable energy has been brought forward as the leading solution to this crisis. Solar and wind energy, specifically, have the most potential to create lasting change in regards to our energy usage. The reality of this situation has brought us to think about modeling different solar variables. Being able to model Global Horizontal Irradiance, for example, allows places around the globe to gain a better understanding of whether or not they would be suitable for solar energy. Qualitative measures such as economic feasibility, the physical geography of places, and political and regulatory hurdles are other things that places can take into account in order to determine if solar is feasible for them, in addition to considering this analysis (Crook et al. 2011; Wild, Folini, and Henschel 2017; Boland 2020; Shahsavari and Akbari 2018). This leads us into our two research questions:\n\nHow well can we model Global Horizontal Irradiance using a time series approach for different situations across the world? And,\nAre the models we create consistent across the 25 year period (1998-2023) for which we have data from?\n\n\n\nThis project focuses on Global Horizontal Irradiance (GHI), a measure of the power of the power of sunlight that reaches the horizontal ground (Stein, Hansen, and Reno 2012). GHI can then be used to estimate the output of a hypothetical solar installation placed on the ground. GHI data used in this report is part of a publicly available GHI data set from SolarAnywhere. SolarAnywhere uses data from geosynchronous satellites to estimate GHI rather than measuring it directly with ground stations (“Historical Data Validation” 2023). Their model is calibrated with data from ground stations. Annual data is accurate within \\(\\pm\\) 4.2% based on this validation. Errors increase for shorter time periods: monthly data has an RMSE of 3.9% and hourly data has an RMSE of 20.6%.\nWe focus our analysis on Victoria, Seychelles, with comparisons to Nairobi, Kenya, and Manchester, England. Nairobi and Victoria are at similar latitudes and so receive a similar amount of sun throughout the year but have different climates. Manchester’s sunlight is more seasonal because of its latitude and it has a very different climate.\nWe use daily Solar Insolation data in our analysis of daily GHI. Solar Insolation in our data is the amount of downward solar energy incident at the top of the atmosphere (Hartmann 2016). It varies by latitude and time of year. We generate daily insolation data for each of our selected locations using the Python package climlab (Rose et al. 2022). The insolation data has units of \\(\\frac{W}{m^2}\\) per hour. Hourly GHI data has also has units of \\(\\frac{W}{m^2}\\) per hour. The difference is that GHI is measured on the ground and solar insolation is measured at the top of the atmosphere."
  },
  {
    "objectID": "projects/correlated/index.html#model-discussion-and-justification",
    "href": "projects/correlated/index.html#model-discussion-and-justification",
    "title": "A Time Series Approach to Modeling Potential Solar Output",
    "section": "Model Discussion and Justification",
    "text": "Model Discussion and Justification\n\nIntra-Day Variation in GHI\nTo begin, we wanted to model the intra-day variation in GHI across our sites, and across the 25 year period. An overview of the process is as follows: First, we found the best fitting sine curve to model the average GHI by month for a given 5-year period. Next, we extrapolated this curve onto the plot of ALL data points (no longer mean GHI, but GHI). Then, we used a GEE model to model the residuals of our sine curve model, which determined what order AR model to use. We used five different 5-year periods in order to see if there was any changes in our model across time. For the purposes of this explanation, we will show what the process looked like for the years 1998-2002 across each of our three locations.\nFor the first step of our process, we decided to model GHI throughout a day by using a sine curve. We made this choice based on striking similarities between the shape of our distribution and that of a sine curve. The equation for the sine curve we chose is listed below: \\(curveFit = \\sin(\\pi(time - P)/24)\\) time is an integer measured from 0 to 24, representing an hour of a given day. P is a parameter used to phase shift the sine curve left or right. This equation is consistent across all three locations except for P. We put the results of curveFit into the linear model below: \\(MeanGHI \\sim curveFit*Night*factor(Month)\\)\nwhere Month is a categorical variable for the month of the year and Night is an indicator variable that says whether or not it is nighttime, i.e. the GHI is zero. Night is coded differently for each location. For the Seychelles, since it’s latitude is only 4 degrees south of the equator, it did not have much variation in terms of the length of daylight during a day. As such, Night could be thought of as between 8 in the morning and 6 at night. For Nairobi, since it’s latitude is very similar to Seychelles’, we treated Night the same. But, for Manchester, as it’s latitude is 53 degrees north of the equator, the length of daytime throughout the year changes drastically. Thus, we changed how we coded Night. For Manchester, if the minimimum GHI for a given month at a given hour is 0, meaning that at least once during a month the GHI is zero at that hour, then we treated that hour as Nighttime. You can see the variation, or lack thereof, in GHI during a day for each month below. Nairobi is pretty similar to Seychelles, and thus is not shown.\n\n\n\n\n\n\n\n\nTo determine phase shift parameter for each location, we found the optimal parameter based on the minimum of the sum of the squared residuals.\n\n\n\n\n\nBased on this scatterplot, we chose a parameter of 0.74 for Victoria Seychelles. Similar plots were done for Nairobi and Manchester, producing a parameter of 1.07 and 0.56, respectively. Now that we have an optimized sine curve model, we fit the to model GHI against hour within the day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompared to both Victoria and Nairobi, Manchester’s plot looks less clean. This is due to the variation in the length of nighttime across each month, as outlined above. Along the same lines, Manchester has a wider distribution due to this variation, with days in the summer months lasting much longer than those in the winter months. Additionally, Manchester’s fitted lines are clustered towards the bottom of the distribution, whereas Victoria’s and Nairobi’s are clustered near the top, indicating the prevalence of low GHI days in Manchester, and high GHI days in Seychelles and Kenya.\n\n\n\n\n\n\n\n\n\n\n\nThe final step in using a time series approach to model the intra-day variation in GHI is to model the correlation between neighboring hours in the day. We do that by looking at the distribution of our residuals from our model, which is basically subtracting our model predictions from the actual values. The next step is to use tools like an autocorrelation function (ACF) and a partial autocorrelation function (PACF) to figure out at what lag does the correlation between a given data point and the lag X (X being what we are trying to find) of that data point drop to 0. This could help us choose the order of an autoregressive or moving average model.\nSuppose in our PACF we saw correlation drop to 0 at lag 3. Then, we would be inclined to use an autoregressive model (AR), with an order of 3. We, however, could not use these methods in order to determine what order AR or MA (moving average) model to use because of the amount of zeros during the nighttime in our data.\nTherefore, we use a longitudinal model called GEE to fit an autoregressive model. After running the GEE model with a max of 8 lags as explanatory variables, and the residuals as the response variable, only the first 4 lags were significant. That is why we used the GEE model; it allowed us to determine which lags were significant in predicting the residual, the data point that the lag is connected too. In essence, it is similar to the PACF as we condition on the intervening lags. Whichever lag is no longer significant is the same as “dropping to 0” in a PACF. To contextualize this a bit, this is saying that, if it is 1 in the afternoon, the GHI at 9 in the morning helps to explain some of the GHI at 1. But, the GHI at 8 in the morning doesn’t quite explain enough of the GHI at 1 for it to be significant enought to include in our model. From this, an AR(4) model was determined to be the most optimal for our data. Interestingly, these conclusions are similar across both Manchester, Nairobi, and Victoria.\nAll the plots above were done on the first 5 years of data, from 1998-2002. We did this same process outlined above for each 5 year segment, for a total of 5 times. The sine curve parameter is the only difference between models across each 5 year period. Shown below is a plot of change in the sine curve parameter over time, setting the 1998-2002 parameter as a benchmark.\n\n\n\n\n\nInterestingly, the phase shift parameter consistently increased from 1998 to 2023 across all 3 locations. Remember that this parameter deals with the shifting of the sine curve left or right, changing when the peak of the curve happens in a day. The increase in parameter means that, from 1998 to 2023, our models’ predicted peak of GHI happened later in the day compared to the 5 year period of 1998 to 2002. This is an interesting observation, because one might think it be possible for the amplitude of the sine curve to increase over time, maybe due to climate change, but where the peak amount of GHI is during the day should fairly stay the same across time.\nNow that we have modeled the intra-day variation in GHI, we next need to explore the seasonal variation in GHI. How does GHI change throughout a year?\n\n\nSeasonal Variation in Daily GHI\nDaily GHI data for the Seychelles has multiple sources of seasonality. The maximum solar energy available changes in a regular pattern because of the rotation of the earth. Seasonal weather patterns influence the amount of solar energy reaching the ground (which is available to solar power installations). Since we have solar insolation data the seasonality in the maximum energy available is easy to account for. If we model the maximum GHI for each day of the year based on solar insolation alone we get a reasonable estimate of maximum GHI.\n\n\n\n\n\n\n\n\n\nTo deal with the first source of seasonality we change our approach. Instead of modeling GHI, we examine the fraction of energy that reaches the ground using the equation:\n\\[\n\\text{Fraction of energy reaching ground} = 1 - \\frac{\\text{Solar Insolation per Day} - \\text{GHI per Day}}{\\text{Solar Insolation per Day}}\n\\]\nWe can now see the effect of seasonal weather patterns more easily. On average, in clear skies, over the entire earth, the atmosphere reflects about 30% of incoming solar energy back into space (Rhodes 2010). In the graph, we can see that the maximum fraction of solar energy that reaches the ground is about 70%. Most days are not perfectly clear though so typically less than 70% of the energy reaches the surface.\nWe estimated the overall trend in the fraction of energy reaching the surface for each location. Each year we expect the fraction of energy reaching the ground to fall by 0.00173 and 0.00168 percentage points in Victoria and Nairobi, respectively, but rise by 0.0008 percentage points in Manchester. Solar insolation does not change significantly over these time frames so the trend we observe is likely a result of climate change and weather.\nThe seasonality not explained by the seasonality of insolation can also be explained by climate and weather: Seychelles receives the most precipitation in January and December, with relatively dry summers (“Climate Change Knowledge Portal” 2023). We decided to model this using a spline with two knots placed to roughly line up with the seasons, this is plotted in blue below. We add this model to the existing trend model.\n\n\n\n\n\n\n\n\n\nManchester also sees rainy winters and so exhibits a similar seasonality (“Climate Change Knowledge Portal” 2023). In Nairobi, the seasonality is not associated with precipitation but rather with low clouds that are present in June, July, and August (Ng’ang’a 1992). This background on the seasons informs our placement of knots for a spline model plotted in blue. The residuals of this approach have significant relationships with humidity, suggesting we should add humidity to our model.\nAverage daily relative humidity lets us model more of the relationship between weather and the energy reaching the ground. Humidity impacts the clear sky GHI (the GHI if there were no clouds) and can give us some sense of cloud cover since the data lacks any direct measure of cloud cover. We incorporate humidity into the three seasonal models as a degree 1 spline with a single knot. This reflects the observation that the unexplained relationship with humidity is relatively flat at lower humidity but is more negative at higher humidity. Kenya and England have their knots at a relative humidity of 60%, the Seychelles has its at 75%. This greatly reduces or eliminates the relationship between the model residuals and humidity.\n\n\n\n\n\n\n\n\n\nIf we examine just a short period in the Seychelles, the first 2 months of 2002, it appears that many of the shorter-term fluctuations in GHI are still present after modeling. These fluctuations seem to be about 2-4 days long which is why the AR models with order 2-4 performed well on various error metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe errors of the combined model with seasonality, humidity, and the overall trend are not white noise, so we attempt to model them. The ACF plot tells us the correlation between errors that are a specific number of lags (measured in days) apart. The ACF decreases as the lags increase. This makes sense because the farther away two points are from one another in time, the weaker the correlation between those two values will be. The PACF plot tells us the correlation between errors and a specific number of lags apart after accounting for the correlation between shorter lags. Values within the dashed lines indicate independent white noise.\n\n\n\n\n\n\n\n\n\nWe come up with five candidate models after looking at the ACF and PACF: an AR3 model, an AR2 model, an AR2MA1 model, an AR3MA1 model, and an AR3MA2 model. These choices are primarily based on the slower decay of the ACF and the PACF being within the dashed lines after 3 lags. Of these, the AR2MA1 model performs the best as measured by AIC (A goodness of fit measure with a penalty for model terms). The Ljung-Box statistic has large p-values, so we fail to reject the null hypothesis that the residuals are independent There is no proof of serial correlation after fitting the model. The AR3 model performed very similarly on all metrics but has slightly worse fit as measured by AIC.\nThis procedure is repeated for Manchester and Nairobi with similar results. Using the same criteria as above we settle on an AR3MA1 model for Manchester and England. In England, the AR4 and AR3 models performed well. In Kenya, the AR3 model also performed well. The actual effects of these models are very slight, but they do handle the correlation present.\n\n\n\n\n\n\n\n\n\nThe Seychelles model residuals have a reasonably insignificant relationship with date, day of year, and humidity. The strong positive relationship between daily GHI and the residual suggests a major issue with the model: we underpredict the fraction of sun reaching the ground on days with higher total GHI and overpredict on days with lower total GHI.\nForecasting with this complete model is not possible without a provided forecast for humidity since we relied on having a known humidity value when creating the model."
  },
  {
    "objectID": "projects/correlated/index.html#discussion-and-conclusions",
    "href": "projects/correlated/index.html#discussion-and-conclusions",
    "title": "A Time Series Approach to Modeling Potential Solar Output",
    "section": "Discussion and Conclusions",
    "text": "Discussion and Conclusions\nResearchers have explored many different ways to model GHI. These methods come down to ways to model weather and cloud cover. Researchers have used an autoregressive spatio-temporal model to model GHI in the short term (Dambreville et al. 2014). The forecasts resulting from that model were just 15 minutes to 1 hour. Yang et al. (2015) uses exponential smoothing to predict cloud cover and, therefore, GHI. Aside from time series and spatial approaches, neural networks have proven quite successful at accounting for the highly complex relationships that impact GHI. Zang et al. (2020) combine two types of neural networks for spatio-temporal predictions. This, of course, sacrifices interpretability.\nUsing a sine curve to model intra-day changes in GHI is a different approach than those outlined above, and given the tools at our disposal, a good approach. The sine curve provided a unique way to capture the distribution of GHI during a day, and did quite well given the almost simplistic nature of it. The most challenging aspect of the intraday GHI was handling when it was nighttime. As discussed above, this involved different approaches to determine what it meant to be nighttime, but more importantly, proved challenging in uses traditional time series techniques to remove trend and model the errors. Nairobi and Victoria both lie on roughly the same latitude, yet the rank of the monthly sine curves (which month had the highest amplitude) was quite different when comparing the two. This indicates that weather patterns are significantly different for Nairobi and Victoria, as well as that there isn’t a whole lot of noticeable difference between months, leading to the rank of each month to be different than what we would expect given a traditional seasonal approach. Unsurprisingly, Manchester’s GHI was significantly lower than both Victoria’s and Nairobi’s. In the winter months of December and Janurary, the amplitude of the curve was significantly lower than the corresponding “winter” months for Victoria and Nairobi.\nOur approach to modeling the daily sunlight highlights the incredible variation between different locations and the difficulty of accurately forecasting weather conditions. When first converting from GHI to our metric, the fraction of sunlight reaching the ground, each location was at a different latitude and required a different set of solar insolation data. This solar insolation data simplifies the modeling process but there was still a lot to model. The overall trend in the fraction of sunlight reaching the ground, which has been roughly linear, is also different for each of the three locations. Each location experienced different climate and yearly weather patterns and required separate spline models for the time of year. Even the relationship between relative humidity and the fraction of sunlight reaching the ground was unique between the locations. Residuals after all this modeling showed correlation for data 2 to 4 days apart. That is just a result of how the weather tends to work from day to day. Each of the locations showed relatively similar results for this part of the modeling.\nThe overall trend we saw in daily sunlight in the Seychelles could be a part of general trends in cloud cover in the Area. Sharma et al. (2023) shows that cloud cover in the Arabian Sea (north of the Seychelles) has started to increase and predicts that it will continue to increase as climate change continues. These results are not generalizable as they find the opposite trend in the South-East Indian Ocean.\nOverall, the model performed poorly. Lacking a solid proxy for cloud cover in the data, we had little way to predict the day-to-day changes in cloud cover. Of course, modeling would be very straightforward if we had a measure of cloud cover. The fraction of sunlight reaching the ground is directly linked to cloud cover. That would also force us to rely on some outside cloud cover forecast if we wanted to do any forecasting. In existing work, spatial data and spatial models play a critical role. Weather is notoriously difficult to predict and this difficulty is amplified when the data you have is for single locations hundreds of miles apart."
  },
  {
    "objectID": "projects/asteroids/index.html",
    "href": "projects/asteroids/index.html",
    "title": "Asteroid Value Analysis",
    "section": "",
    "text": "A report that visualizes the location, size, composition, and value of asteroids in the Asterank dataset. Created in collaboration with Aaliyah Dick and Courtney Brown."
  },
  {
    "objectID": "projects/asteroids/index.html#background",
    "href": "projects/asteroids/index.html#background",
    "title": "Asteroid Value Analysis",
    "section": "Background",
    "text": "Background\nAfter the Moon Landing in 1969, interest in the universe beyond Earth surged and one of many questions raised was that of asteroid mining. NASA’s Jet Propulsion Laboratory maintains a database called the Small Bodies Database (SBDB) containing a record of small Solar System bodies, which includes any celestial body in our Solar System other than planets and natural satellites (e.g. moons). In 2012, a new database called Asterank was created using the SBDB, among other sources. New variables include the spectral type, value, and profit of each asteroid, which are critical for mining. Most of the following analysis is based on a subset of only asteroids with defined spectral types in Asterank’s data."
  },
  {
    "objectID": "projects/asteroids/index.html#definitions-and-setup",
    "href": "projects/asteroids/index.html#definitions-and-setup",
    "title": "Asteroid Value Analysis",
    "section": "Definitions and Setup",
    "text": "Definitions and Setup\n\nAsteroid\nAsteroids are small rocky, metallic, or icy bodies with no atmosphere orbiting the Sun.\n\n\nAU\nAn Astronomical Unit (AU) is a unit of length roughly equal to the distance from the Earth to the Sun and is about 150 million kilometers (93 million miles).\n\n\nComposition\nAsteroids fall into 3 different composition classes: C-type, S-type, and M-type. C-type, or chondrite, asteroids are the most common and consist of clay and silicate rocks. S-type, or “stony”, asteroids are made of silicate materials and nickel-iron. M-type, or metallic, asteroids have varied composition depending on how far from the Sun they formed. Composition can be estimated using spectral data.\n\n\nSpectral Type\n\nVariable Name: spec_B\nAsteroids are typed based on their emission spectrum, albedo, and color. See https://en.wikipedia.org/wiki/Asteroid_spectral_types for list of spectral types. Essentially, this is based on how objects reflect and absorb different wavelengths of light.\n\n\n\nDelta-v\n\nVariable Name: dv\nDelta-v is used to calculate the accessibility, with lower values indicating easier access. Delta-v is, more specifically, the change in velocity (measured in km/s) required of mining spacecraft in order to make contact with the asteroid. Most NEAs require a delta-v around 6-8 km/s and very few asteroids have a delta-v smaller than 4 km/s.\n\n\n\nMain Belt\nThe main belt, also called the asteroid belt or main asteroid belt, is located between Mars and Jupiter contains many of the asteroids in the Solar System, second only to the Kuiper Belt. It is believed that the main belt have been a planet if not for the gravitational pull of Jupiter in the early Solar System.\n\n\nMOID\n\nVariable Name: moid\nMinimum orbit intersection distance, or MOID, is used to assess potential close approaches and collision risks of astronomical objects. A smaller MOID value, and thus a smaller minimum distance, indicates a higher level of risk (but NOT inevitability). MOID is often measured in AU (defined below) or ld (the distance between the moon an Earth).\n\n\n\nNEAs\nNEA is an abbreviation of Near-Earth Asteroid (q &lt; 1.3 AU). There are four groups of NEAs (Atira, Aten, Apollo, & Amor) based on perihelion distance (q), aphelion distance (Q) and their semi-major axes (a).\n\n\nPerihelion\n\nVariable Name: q (AU)\nThe perihelion is the point in orbit closest to the sun.\n\n\n\nPHAs\nPHA is an abbreviation of Potentially Hazardous Asteroid. These are asteroids that have been determined to be at risk of making a threatening approach to the Earth, more specifically asteroids with MOID ≤ 0.05 AU and H ≤ 22.0.\n\n\nSPK-ID\nThe SPK-ID of an asteroid is its identifier in the JPL Small Body Database.\n\n\nHow Asterank Calculates Value and Profit\nWe need to understand how Asterank estimates price and profit before we can do any analysis.\nSince Asterank’s code is available publicly on github we can analyze exactly how it arrives at its predictions for value and profit. Asterank first assigns composition estimates to asteroids based on their spectral type. For example, a type Cg asteroid is .2% water and .166% iron (and other elements). It then uses either estimates of mass in existing literature or estimates of diameter to calculate how much of each element is in each asteroid by weight. Mapping these to material values in $/kg, Asterank calculates the total value for each asteroid. Using calculated delta-v values Asterank estimate the cost of getting to an asteroid and bringing it back to lunar orbit. Profit is calculated with price and cost.\nSide note: Objects would be placed in lunar orbit so that there would be no chance of them impacting Earth.\nThe final value is a product of estimates on estimates on estimates. For the most part, We only have calculated values for asteroids with spectral data. These account for around 5,200 of the nearly 800,000 objects in Asterank’s data. And only 996 have a price greater than 1 cent. Aside from simply unavailable spectral data the biggest issue is estimating composition. There have only been a couple of successful asteroid sample return missions and even less is known about the interior of these objects, not to mention the composition variation from asteroid to asteroid that Asterank cannot account for."
  },
  {
    "objectID": "projects/asteroids/index.html#analysis",
    "href": "projects/asteroids/index.html#analysis",
    "title": "Asteroid Value Analysis",
    "section": "Analysis",
    "text": "Analysis\n\nLocating Value and Profit\n\n\n\n\n\nLet’s start by taking a look at where in the solar system asteroids are along with their value. The many parameters needed to define an objects orbit in the solar system make visualizing location complex. We simplify this by defining location as the distance to the sun when the object is closest to the sun. This is called the perihelion and is measured in Astronomical units (AU) where 1 AU is the approximate distance from the earth to the sun. An object with a highly elliptical orbit which passes though Earth’s orbit at its closest point and Jupiter’s orbit at its furthest point would appear on this plot at Earth. For these highly elliptical objects this visualization clearly does not work well. The eccentricity of an orbit is a measure of how elliptical the orbit is. Lower eccentricities indicate a more circular orbit. High eccentricity in an orbit means that there is a wider range (a.k.a. less consistency) of distance between the asteroid and the Sun. For reference, Earth has an eccentricity of 0.016 and Pluto has an eccentricity of 0.25. The median eccentricity of objects in the full Asterank data is 0.1504736. So it is probably okay just to use the closest point to the sun for our purposes but a more scientific approach would need more orbit parameters.\nAsteroids in our data are the most dense just past the orbit of Mars before Jupiter. This area is the asteroid belt. Asteroids that travel closer to the sun than Mars does have typical values less than one hundred trillion and often less than one trillion (note that the y-axis has been log transformed). In the asteroid belt and beyond, asteroids have calculated values far beyond one trillion, going into quadrillions and quintillions. The asteroids in these regions of space are orders of magnitude apart in terms of price. The size of this difference allows us to be confident in the relationship even if Asterank’s predictions are not exceedingly accurate.\n\n\n\n\n\n\nLooking at individual asteroids with spectral type we see that high profit asteroids tend to be type C and not very close to earth. There are a number of lower profit X type asteroids closer to Earth.\nSo, price is not everything, it costs an incredible amount to get to asteroids and bring them back to earth. Just because an asteroid has a high value does not mean it will be profitable or feasible to mine.\n\n\nDelta-V and Accessibility\nOne priority in determining asteroid profit is the ease of access. Delta-v (Δv) is used to calculate the accessibility, with lower values indicating easier access. Specifically, Δv is the change in velocity (measured in km/s) required of mining spacecraft in order to make contact with the asteroid. But we cannot decide what to mine based on Δv alone. The asteroids named “2018 AV2” and “2009 HC82”, for example, have the smallest and largest Δvs in the dataset (3.74 and 51.19 km/s respectively) but have estimated prices of $0.\n\n\n\n\n\n\n\n\n\n\nThe asteroids in this data subset are massive. In 2012 scientists from NASA and a variety of educational institutions wrote a report on the feasibility of asteroid retrieval. They came up with a plan using current technology and technology in development to bring a 7-meter, 500 ton asteroid into lunar orbit. They estimated the cost at more than 2 billion dollars. In the plot above a red line is placed at 7 meters. None of the asteroids in the subset of data we are working with that have defined diameters fall below this line. It is not currently feasible to bring any of them to the moon.\n\n\n\n\n\n\n\n\n\nThis paper did identify several potential candidates for capture and return. And, for NASA, the real profitability is in science and having access to materials in space. 500 tons of asteroid are 500 tons of material that do not need to be launched into space with expensive rockets.\n\n\n\n\n\nAsterank still sees these asteroids as quite profitable, if not currently feasible. The graph of estimated profit is strikingly similar to the graph of price. Even considering the higher cost of getting to the main belt it is still worthwhile. Expenses are considerable though, while the profit plot has a similar shape it is shifted down considerably. For the median asteroid in our data, expenses eat up 92% of the asteroids value.\nAll of this is hypothetical. There is not country or company that has or is about to capture, return, and mine an asteroid. And certainly not any of the large, high value asteroids in the main belt. Estimates for price and composition are just estimates and more research is needed to clarify them. Right now the focus is on gathering information on asteroids and Asterank is a great tool for generating interest and support for that research.\nNotably, NASA’s OSIRIS-REx mission is on track to return an asteroid sample to Earth in 2023. This, like meteorite composition studies, can be used to better link spectral and composition data."
  },
  {
    "objectID": "projects/asteroids/index.html#resources",
    "href": "projects/asteroids/index.html#resources",
    "title": "Asteroid Value Analysis",
    "section": "Resources",
    "text": "Resources\nAsterank\nAsteroids - Wikipedia\nBasic Asteroid Info - NASA\nObject Classifications - Planetary Data System\nSpectral Types - Wikipedia\nAsteroid Retrieval Feasibility Study\nOSIRIS-REx"
  },
  {
    "objectID": "projects/asteroids/index.html#appendix",
    "href": "projects/asteroids/index.html#appendix",
    "title": "Asteroid Value Analysis",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathaniel Reimer",
    "section": "",
    "text": "he/him/his\nStudent @ Macalester College\n\n\nMacalester College in St. Paul MN  Major Data Science, Minors in Economics and Classical Languages| Sept 2020 - May 2024"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nathaniel Reimer",
    "section": "",
    "text": "Macalester College in St. Paul MN  Major Data Science, Minors in Economics and Classical Languages| Sept 2020 - May 2024"
  },
  {
    "objectID": "projects/mathmodeling/index.html",
    "href": "projects/mathmodeling/index.html",
    "title": "Advection Diffusion with Simple Erosion and Deposition",
    "section": "",
    "text": "Pollutant runoff is harmful to many ecosystems. Agricultural runoff can be particularly harmful to ecosystems because it contains a high concentration of nitrogen. This nitrogen is primarily from nitrogen fertilizers vital to global food supplies. When nitrogen is allowed to enter oceans and waterways it can fuel phytoplankton blooms (Michael Beman, Arrigo, and Matson 2005). Phytoplankton blooms, particularly those involving toxic species, can devastate ecosystems and the people that live near and rely on those ecosystems (Shumway, Burkholder, and Morton 2018). There are a variety of mitigation methods for reducing the amount of nitrogen that is carried out of agricultural areas by runoff."
  },
  {
    "objectID": "projects/mathmodeling/index.html#introduction",
    "href": "projects/mathmodeling/index.html#introduction",
    "title": "Advection Diffusion with Simple Erosion and Deposition",
    "section": "",
    "text": "Pollutant runoff is harmful to many ecosystems. Agricultural runoff can be particularly harmful to ecosystems because it contains a high concentration of nitrogen. This nitrogen is primarily from nitrogen fertilizers vital to global food supplies. When nitrogen is allowed to enter oceans and waterways it can fuel phytoplankton blooms (Michael Beman, Arrigo, and Matson 2005). Phytoplankton blooms, particularly those involving toxic species, can devastate ecosystems and the people that live near and rely on those ecosystems (Shumway, Burkholder, and Morton 2018). There are a variety of mitigation methods for reducing the amount of nitrogen that is carried out of agricultural areas by runoff."
  },
  {
    "objectID": "projects/mathmodeling/index.html#a-potential-model",
    "href": "projects/mathmodeling/index.html#a-potential-model",
    "title": "Advection Diffusion with Simple Erosion and Deposition",
    "section": "A Potential Model",
    "text": "A Potential Model\nWe represent nitrogen dissolved in water with \\(C\\) and nitrogen deposited on the ground with \\(N\\). Nitrogen in water is subject to simple advection-diffusion: \\(C_{t} = -(Cv)_{x} + kC_{xx}\\). Nitrogen on the ground is stationary. Some hydrological models of pollutant transport use similar logical frameworks. Shao et al. use an equivalent exchange layer to mediate between surface runoff and nutrients stored in the ground (Shao et al. 2023). A fractional mobile-immobile model is similarly predicated on simple advection-diffusion with mobile and immobile components of the solute (Sun et al. 2020). Both models present complicated methods for transfer between the two states.\n\\[\nC_{t} = -(Cv)_{x} + kC_{xx} + N \\frac{e^{v-n}}{1 + e^{v-n}} - C\\frac{e^{n-v}}{1 + e^{n-v}}\\\\  \n\\]\n\\[\nN_{t} = - N \\frac{e^{v-n}}{1 + e^{v-n}} + C\\frac{e^{n-v}}{1 + e^{n-v}}\\\\\n\\]\nWe represent transfers between ground nitrogen and dissolved nitrogen in this model with \\(N \\frac{e^{v-n}}{1 + e^{v-n}} - C\\frac{e^{n-v}}{1 + e^{n-v}}\\). For fixed levels of \\(N\\) and \\(C\\), a velocity \\(v &gt; n\\) will lead to erosion. For \\(v &gt;&gt; n\\), the rate of erosion approaches \\(N\\). On the other hand, if velocity \\(v &lt; n\\) deposition will occur. For \\(v &lt;&lt; n\\), the rate of deposition approaches \\(C\\). This ensures that no deposition will occur if the dissolved concentration is zero and no erosion will occur if the concentration on the ground is zero.\nThe simplicity of the model, of course, comes at the expense of accuracy.\nThe model assumes that there is a single erosion-deposition threshold (\\(n\\)); that advection does not vary with time; that the quantity of water does not have an impact; that raindrops do not have an impact; and that water only interacts with nitrogen. In the real world, we would likely see two thresholds for erosion-deposition (Earle 2015). Raindrops, as seen in the equivalent exchange layer model, are big drivers of sediment transport. Also, in the real world, water carrying other things would impact its ability to pick up nitrogen.\n\nImplementation with Scikit FiniteDiff\nA finite difference scheme lets us see the behavior of the model easily. Using the Scikit Finite Difference package we can enter the equations symbolically, set boundary conditions, set initial conditions, and run simulations. This greatly simplifies the modeling workflow.\n\nbc = {(\"C\", \"x\"): (\"dirichlet\", \"dirichlet\"),\n      (\"N\", \"x\"): (\"dirichlet\", \"dirichlet\")}\n\nmodel = Model([\"-dx(C*v) + k*(dxxC) + N*e**(v-n)/(1+e**(v-n)) - C*e**(n-v)/(1+e**(n-v))\",\n               \"-N*e**(v-n)/(1+e**(v-n)) + C*e**(n-v)/(1+e**(n-v))\"],\n              [\"C(x)\", \"N(x)\"],\n              parameters=[\"v(x)\",\"k\",\"n\",\"e\"],\n              boundary_conditions = bc)"
  },
  {
    "objectID": "projects/mathmodeling/index.html#model-behavior",
    "href": "projects/mathmodeling/index.html#model-behavior",
    "title": "Advection Diffusion with Simple Erosion and Deposition",
    "section": "Model Behavior",
    "text": "Model Behavior\n\n\n\n\n\n\n\n\n\nThe basic model behavior is working as intended. Nitrogen from the ground is picked up from areas where velocity is high and deposited in the area where velocity is low.\n\nModeling with Different Advection\nTo get a better grasp on the behavior of the model we run it with a variety of different initial conditions. Here we look at a less effective runoff prevention measure that does not slow down the water as much. In the original, the speed of advection is slowed to .5 at \\(x = 8\\), in this second run it only slows to a speed of 2. Looking at the y-axes of these plots compared to those above we can see that this has managed to catch much less of the nitrogen.\n\n\n\n\n\n\n\n\n\nThese results are better visualized by looking at the total nitrogen present in the simulation over time. Nitrogen that is carried over the right boundary by the water is lost as runoff and we assume it enters the ecosystem. Good runoff mitigation will keep the total nitrogen present in the simulation high so it does not have a chance to enter the ecosystem and cause harmful algae blooms.\n\n\n\n\n\n\n\n\n\nDifferent runoff prevention measures are represented by different advection slowdowns at \\(x = 8\\). The first graph represents no runoff prevention (a speed of 10 at \\(x = 8\\)) and the last graph represents an advection speed of 1 at \\(x = 8\\). In each scenario, there is a period at the start where the nitrogen level declines very slowly. During this period most of the nitrogen simply has not had enough time to advect to the right boundary. This is followed by a period of rapid decline in nitrogen content that slowly tapers down until there is no nitrogen left. The lower graphs, which have the more significant runoff prevention measure, show a slower nitrogen loss.\nThe way the model is set up, as long as there is no area of zero velocity, the total nitrogen content will eventually reach zero. For example: if the erosion deposition threshold \\(n\\) is 5 and the velocity over the entire area is 4 then, \\[\nN_{t} = - N \\frac{e^{-1}}{1 + e^{-1}} + C\\frac{e^{1}}{1 + e^{1}}\\\\\n\\]\nAt a glance, this implies deposition but eventually, any positive \\(C\\) is advected over the right boundary, and \\(C = 0\\) so there can’t be deposition. Even though \\(\\frac{e^{-1}}{1 + e^{-1}}\\) is small it is the only term left so there will actually be erosion. If \\(C = 0\\) and \\(N &gt; 0\\) there will be erosion regardless of velocity. \\(n\\) is not a hard cutoff where erosion becomes deposition, instead, it is the velocity where, assuming equal quantities of \\(C\\) and \\(N\\), erosion and deposition are equal. As \\(v\\) decreases, deposition just becomes a more dominant part of the model even though both deposition and erosion are still happening.\nIn the real world, it does not rain forever though, so slowing down the rate of nitrogen loss is still important.\nI also looked at the effect of widening the low-velocity runoff mitigation area and found similar results.\n\n\n\n\n\n\n\n\n\n\n\nAnomalous Solute Transport\nModeling anomalous solute transport is important for understanding water pollution. In normal transport, solute moves consistently with some linear relationship to time. Anomalous transport typically involves the early or late arrival of a portion of the solute. It tends to occur in porous mediums and when there are multiple different paths for the solute to take. This is often modeled with fractional models as is discussed in Sun et al. (2020). As seen earlier, the model developed here can replicate some of these characteristics. Most of the nitrogen reaches the right boundary of the simulation fairly early but small amounts of solute continue to arrive for a long time afterwards.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can show the anomalous transport even better with a different initial condition. We set up alternating areas of fast and slow transport and a uniform initial distribution of nitrogen on the ground. This results in lots of nitrogen leaving the simulation at the start followed by smaller amounts of late-arriving nitrogen. When the simulation begins some of the nitrogen gets trapped in areas of low advection. Once trapped, it prevents other nitrogen from becoming trapped because a large enough \\(N\\) can offset the deposition that occurs when advection speed is below \\(n\\). So, most of the nitrogen is carried to the end without getting trapped. Then, as the water becomes free of nitrogen there is no longer any deposition to offset, so the large \\(N\\) just leads to erosion. The trapped nitrogen is slowly eroded and exits the simulation. This lines up very well with the theory of anomalous transport, which can often involve things like traps or side pools where the solute is stored temporarily."
  },
  {
    "objectID": "projects/mathmodeling/index.html#future-work",
    "href": "projects/mathmodeling/index.html#future-work",
    "title": "Advection Diffusion with Simple Erosion and Deposition",
    "section": "Future Work",
    "text": "Future Work\nThere is so much I would be interested in pursuing more if I had another two months to continue this project. I would first be interested in developing a more data driven approach to generating the spatially varying advection fields. This might require another mathematical model but it could also be done with an easier statistical approach using the data presented in Liu et al. (2020).\nAnother version of the model I used briefly had two different thresholds which more accurately matches how erosion and deposition work in the real world. The behavior of that model was extremely similar to the current model but I think it would be interesting to set up a couple of simulations to explore how they differ more methodically.\nImplementing this model in 3D would be very straightforward using skfdiff and is also something I would want to look into. I think the main issue would be capturing how water moves around obstacles in the real world.\nMoving away from just looking at nitrogen runoff I am interested in exploring what behavior results if velocity is made to depend on \\(N\\). \\(N\\), in that case, might represent something like soil that slows down water but can also be picked up and carried by it."
  },
  {
    "objectID": "projects/doublerobust/index.html",
    "href": "projects/doublerobust/index.html",
    "title": "Doubly Robust Estimators",
    "section": "",
    "text": "Doubly Robust estimation allows researchers to take advantage of both an outcome model and a model of treatment. If one or both of the models is correct, the doubly robust estimator will produce an accurate estimate of the causal effect. Here I’ll focus on doubly robust estimation for a quantitative outcome and a binary treatment. Here is one such estimator:\n\\[\\begin{aligned}\n&\\frac{1}{n}\\sum \\left[ \\hat{y}_1(\\textrm{covariates}_i) + \\frac{Y_i A_i}{\\hat{e}(\\textrm{covariates}_i)} - \\frac{\\hat{y}_1(\\textrm{covariates}_i)*A_i}{\\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n-&\\frac{1}{n}\\sum \\left[ \\hat{y}_0(\\textrm{covariates}_i) + \\frac{Y_i(1 - A_i) }{1 - \\hat{e}(\\textrm{covariates}_i)} - \\frac{\\hat{y}_0(\\textrm{covariates}_i)*(1 - A_i)}{1 - \\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{A}) \\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{B}) \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{C})\n\\end{aligned}\\]\nOr, alternatively:\n\\[\\begin{aligned}\n(\\textrm{A})  \\hskip 2em &\\frac{1}{n}\\sum \\left[ \\hat{y}_1(\\textrm{covariates}_i) - \\hat{y}_0(\\textrm{covariates}_i) \\right]\\\\\n\\\\\n(\\textrm{B}) \\hskip 1.2em +&\\frac{1}{n}\\sum \\left[ \\frac{Y_i A_i}{\\hat{e}(\\textrm{covariates}_i)} - \\frac{Y_i(1 - A_i) }{1 - \\hat{e}(\\textrm{covariates}_i)} \\right]\\\\\n\\\\\n(\\textrm{C}) \\hskip 1.2em -&\\frac{1}{n}\\sum \\left[\\frac{\\hat{y}_1(\\textrm{covariates}_i)*A_i}{\\hat{e}(\\textrm{covariates}_i)} -  \\frac{\\hat{y}_0(\\textrm{covariates}_i)*(1 - A_i)}{1 - \\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n\\\\\n\\end{aligned}\\]\nWhere\n\n\\(\\hat{y}_1(\\textrm{covariates}_i)\\) are the predicted values for each individual \\(i\\) with treatment \\(1\\).\n\\(\\hat{y}_0(\\textrm{covariates}_i)\\) are the predicted values for each individual \\(i\\) with treatment \\(0\\).\n\\(Y_i\\) and \\(A_i\\) are, respectively, the actual outcome and treatment values for each individual.\n\\(\\hat{e}(\\textrm{covariates}_i)\\) are the predicted propensity scores for each individual \\(i\\).\n\nThe rest of this page will attempt to explain how the doubly robust property works in this estimator."
  },
  {
    "objectID": "projects/doublerobust/index.html#sources",
    "href": "projects/doublerobust/index.html#sources",
    "title": "Doubly Robust Estimators",
    "section": "Sources",
    "text": "Sources\n\n“Doubly Robust Estimation of Causal Effects” by Funk et al. and “A Falsifiability Characterization of Double Robustness Through Logical Operators” by Frangakis were used heavily in this project. My goal in this project was essentially to create an explanation of double robustness that combined Frangakis’ logical framework with the more numerical and simulation-based approach of the Funk paper.\nDoubly Robust Estimation of Causal Effects\nA Falsifiability Characterization of Double Robustness Through Logical Operators"
  },
  {
    "objectID": "projects/doublerobust/index.html#code",
    "href": "projects/doublerobust/index.html#code",
    "title": "Doubly Robust Estimators",
    "section": "Code",
    "text": "Code\n\n\nSimulating Data\n\nset.seed(7)\nn &lt;- 100000\nZ &lt;- rnorm(n, 1, 2) # Z\n\nW &lt;- rbinom(n, 1, .5) # W\n\nX &lt;- rnorm(n, 2*W, 2) # X\n\nM &lt;- rnorm(n, 20, 1) # M\n\nN &lt;- rnorm(n, 10, 2) # N\n\nlog_odds_A &lt;- .25 * Z + .5 * X + .1 * N\nodds_A &lt;- exp(log_odds_A)\np_A &lt;- odds_A / (1 + odds_A)\nA &lt;- rbinom(n, 1, p_A) # A\n\n\n\nY &lt;- rnorm(n, A + W + 2 * Z + 0.05 * M + rnorm(n, 2, 1), .25) # Y\n\ndata &lt;- data.frame(A = (A), Y = Y, Z = Z, M = M, W = (W), X = X)\n\n\n\nFunction for Normal Methods\n\nget_normal_methods &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n  \n  \n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n  \n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n  # print(summary(example_ipw)$coefficients)\n  # print(summary(outcome)$coefficients)\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2]))\n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\",\"Causal Effect Estimate from Outcome Model\", \"Causal Effect Estimate from Treatment Model\")\n  return(output)\n}\n\n\n\nFunction for Two Parts of DR Estimator\n\nget_dr_parts2_3 &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n  \n  forceA0 &lt;- data %&gt;% mutate(A = 0)\n  forceA1 &lt;- data %&gt;% mutate(A = 1)\n\n  predA0 &lt;- predict(outcome, newdata = forceA0)\n  predA1 &lt;- predict(outcome, newdata = forceA1)\n\n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n  part1 &lt;- predA1\n  part2 &lt;- ((data$Y * data$A) / (data$ps))\n  part3 &lt;- -((predA1 * data$A) / data$ps)\n\n\n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2], mean(part2), mean(-part3), mean(part2+part3)))\n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\", \"Outcome Model Estimate\", \"IPW Estimate\", \"$B$\", \"$C$\", \"D\")\n  return(output)\n}\n\n\n\nFunction for All Parts of DR Estimator\n\nget_dr_parts &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n\n\n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n\n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n\n  forceA0 &lt;- data %&gt;% mutate(A = 0)\n  forceA1 &lt;- data %&gt;% mutate(A = 1)\n\n  predA0 &lt;- predict(outcome, newdata = forceA0)\n  predA1 &lt;- predict(outcome, newdata = forceA1)\n\n  DR1 &lt;-\n    (((    data$A) * data$Y) / (    data$ps)) -      # IPW part\n    ((predA1 * (data$A - data$ps))) / (    data$ps)  # Other thing\n  DR0 &lt;-\n    (((1 - data$A) * data$Y) / (1 - data$ps)) +      # IPW part\n    ((predA0 * (data$A - data$ps))) / (1 - data$ps)  # Other thing\n\n  Estimator1 &lt;- predA1 + ((data$Y * data$A) / (data$ps)) - ((predA1 * data$A) / data$ps)\n  Estimator0 &lt;- predA0 + ((data$Y * (1 - data$A)) / (1 - data$ps)) - ((predA0 * (1 - data$A)) / (1 - data$ps))\n\n\n  part1 &lt;- predA1\n  part2 &lt;- ((data$Y * data$A) / (data$ps))\n  part3 &lt;- -((predA1 * data$A) / data$ps)\n  \n  part1A0 &lt;- predA0\n  part2A0 &lt;- ((data$Y * (1 - data$A)) / (1 - data$ps))\n  part3A0 &lt;- -((predA0 * (1 - data$A)) / (1 - data$ps))\n  \n  partdr1 &lt;- mean(part1) - mean(part1A0)\n  partdr2 &lt;- mean((data$Y * data$A) / (data$ps)) - mean((data$Y * (1 - data$A)) / (1 - data$ps))\n  partdr3 &lt;- mean(-((predA1 * data$A) / data$ps)) - mean(-((predA0 * (1 - data$A)) / (1 - data$ps)))\n\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2], partdr1, partdr2, partdr3, sum(partdr1, partdr2, partdr3)))\n  \n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\", \"Outcome Model Estimate\", \"IPW Estimate\", \"$B$\", \"$C$\", \"D\", \"E\")\n  return(output)\n}"
  },
  {
    "objectID": "projects/definingachievement/index.html",
    "href": "projects/definingachievement/index.html",
    "title": "Defining Achievement",
    "section": "",
    "text": "Link to Project Site"
  }
]