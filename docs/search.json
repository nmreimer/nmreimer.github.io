[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathaniel Reimer",
    "section": "",
    "text": "he/him/his\nStudent @ Macalester College\n\n\nMacalester College in St. Paul MN  Major Data Science, Minors in Economics and Classical Languages| Sept 2020 - May 2024"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nathaniel Reimer",
    "section": "",
    "text": "Macalester College in St. Paul MN  Major Data Science, Minors in Economics and Classical Languages| Sept 2020 - May 2024"
  },
  {
    "objectID": "projects/asteroids/index.html",
    "href": "projects/asteroids/index.html",
    "title": "Asteroid Value Analysis",
    "section": "",
    "text": "A report that visualizes the location, size, composition, and value of asteroids in the Asterank dataset. Created in collaboration with Aaliyah Dick and Courtney Brown."
  },
  {
    "objectID": "projects/asteroids/index.html#background",
    "href": "projects/asteroids/index.html#background",
    "title": "Asteroid Value Analysis",
    "section": "Background",
    "text": "Background\nAfter the Moon Landing in 1969, interest in the universe beyond Earth surged and one of many questions raised was that of asteroid mining. NASA’s Jet Propulsion Laboratory maintains a database called the Small Bodies Database (SBDB) containing a record of small Solar System bodies, which includes any celestial body in our Solar System other than planets and natural satellites (e.g. moons). In 2012, a new database called Asterank was created using the SBDB, among other sources. New variables include the spectral type, value, and profit of each asteroid, which are critical for mining. Most of the following analysis is based on a subset of only asteroids with defined spectral types in Asterank’s data."
  },
  {
    "objectID": "projects/asteroids/index.html#definitions-and-setup",
    "href": "projects/asteroids/index.html#definitions-and-setup",
    "title": "Asteroid Value Analysis",
    "section": "Definitions and Setup",
    "text": "Definitions and Setup\n\nAsteroid\nAsteroids are small rocky, metallic, or icy bodies with no atmosphere orbiting the Sun.\n\n\nAU\nAn Astronomical Unit (AU) is a unit of length roughly equal to the distance from the Earth to the Sun and is about 150 million kilometers (93 million miles).\n\n\nComposition\nAsteroids fall into 3 different composition classes: C-type, S-type, and M-type. C-type, or chondrite, asteroids are the most common and consist of clay and silicate rocks. S-type, or “stony”, asteroids are made of silicate materials and nickel-iron. M-type, or metallic, asteroids have varied composition depending on how far from the Sun they formed. Composition can be estimated using spectral data.\n\n\nSpectral Type\n\nVariable Name: spec_B\nAsteroids are typed based on their emission spectrum, albedo, and color. See https://en.wikipedia.org/wiki/Asteroid_spectral_types for list of spectral types. Essentially, this is based on how objects reflect and absorb different wavelengths of light.\n\n\n\nDelta-v\n\nVariable Name: dv\nDelta-v is used to calculate the accessibility, with lower values indicating easier access. Delta-v is, more specifically, the change in velocity (measured in km/s) required of mining spacecraft in order to make contact with the asteroid. Most NEAs require a delta-v around 6-8 km/s and very few asteroids have a delta-v smaller than 4 km/s.\n\n\n\nMain Belt\nThe main belt, also called the asteroid belt or main asteroid belt, is located between Mars and Jupiter contains many of the asteroids in the Solar System, second only to the Kuiper Belt. It is believed that the main belt have been a planet if not for the gravitational pull of Jupiter in the early Solar System.\n\n\nMOID\n\nVariable Name: moid\nMinimum orbit intersection distance, or MOID, is used to assess potential close approaches and collision risks of astronomical objects. A smaller MOID value, and thus a smaller minimum distance, indicates a higher level of risk (but NOT inevitability). MOID is often measured in AU (defined below) or ld (the distance between the moon an Earth).\n\n\n\nNEAs\nNEA is an abbreviation of Near-Earth Asteroid (q &lt; 1.3 AU). There are four groups of NEAs (Atira, Aten, Apollo, & Amor) based on perihelion distance (q), aphelion distance (Q) and their semi-major axes (a).\n\n\nPerihelion\n\nVariable Name: q (AU)\nThe perihelion is the point in orbit closest to the sun.\n\n\n\nPHAs\nPHA is an abbreviation of Potentially Hazardous Asteroid. These are asteroids that have been determined to be at risk of making a threatening approach to the Earth, more specifically asteroids with MOID ≤ 0.05 AU and H ≤ 22.0.\n\n\nSPK-ID\nThe SPK-ID of an asteroid is its identifier in the JPL Small Body Database.\n\n\nHow Asterank Calculates Value and Profit\nWe need to understand how Asterank estimates price and profit before we can do any analysis.\nSince Asterank’s code is available publicly on github we can analyze exactly how it arrives at its predictions for value and profit. Asterank first assigns composition estimates to asteroids based on their spectral type. For example, a type Cg asteroid is .2% water and .166% iron (and other elements). It then uses either estimates of mass in existing literature or estimates of diameter to calculate how much of each element is in each asteroid by weight. Mapping these to material values in $/kg, Asterank calculates the total value for each asteroid. Using calculated delta-v values Asterank estimate the cost of getting to an asteroid and bringing it back to lunar orbit. Profit is calculated with price and cost.\nSide note: Objects would be placed in lunar orbit so that there would be no chance of them impacting Earth.\nThe final value is a product of estimates on estimates on estimates. For the most part, We only have calculated values for asteroids with spectral data. These account for around 5,200 of the nearly 800,000 objects in Asterank’s data. And only 996 have a price greater than 1 cent. Aside from simply unavailable spectral data the biggest issue is estimating composition. There have only been a couple of successful asteroid sample return missions and even less is known about the interior of these objects, not to mention the composition variation from asteroid to asteroid that Asterank cannot account for."
  },
  {
    "objectID": "projects/asteroids/index.html#analysis",
    "href": "projects/asteroids/index.html#analysis",
    "title": "Asteroid Value Analysis",
    "section": "Analysis",
    "text": "Analysis\n\nLocating Value and Profit\n\n\n\n\n\nLet’s start by taking a look at where in the solar system asteroids are along with their value. The many parameters needed to define an objects orbit in the solar system make visualizing location complex. We simplify this by defining location as the distance to the sun when the object is closest to the sun. This is called the perihelion and is measured in Astronomical units (AU) where 1 AU is the approximate distance from the earth to the sun. An object with a highly elliptical orbit which passes though Earth’s orbit at its closest point and Jupiter’s orbit at its furthest point would appear on this plot at Earth. For these highly elliptical objects this visualization clearly does not work well. The eccentricity of an orbit is a measure of how elliptical the orbit is. Lower eccentricities indicate a more circular orbit. High eccentricity in an orbit means that there is a wider range (a.k.a. less consistency) of distance between the asteroid and the Sun. For reference, Earth has an eccentricity of 0.016 and Pluto has an eccentricity of 0.25. The median eccentricity of objects in the full Asterank data is 0.1504736. So it is probably okay just to use the closest point to the sun for our purposes but a more scientific approach would need more orbit parameters.\nAsteroids in our data are the most dense just past the orbit of Mars before Jupiter. This area is the asteroid belt. Asteroids that travel closer to the sun than Mars does have typical values less than one hundred trillion and often less than one trillion (note that the y-axis has been log transformed). In the asteroid belt and beyond, asteroids have calculated values far beyond one trillion, going into quadrillions and quintillions. The asteroids in these regions of space are orders of magnitude apart in terms of price. The size of this difference allows us to be confident in the relationship even if Asterank’s predictions are not exceedingly accurate.\n\n\n\n\n\n\nLooking at individual asteroids with spectral type we see that high profit asteroids tend to be type C and not very close to earth. There are a number of lower profit X type asteroids closer to Earth.\nSo, price is not everything, it costs an incredible amount to get to asteroids and bring them back to earth. Just because an asteroid has a high value does not mean it will be profitable or feasible to mine.\n\n\nDelta-V and Accessibility\nOne priority in determining asteroid profit is the ease of access. Delta-v (Δv) is used to calculate the accessibility, with lower values indicating easier access. Specifically, Δv is the change in velocity (measured in km/s) required of mining spacecraft in order to make contact with the asteroid. But we cannot decide what to mine based on Δv alone. The asteroids named “2018 AV2” and “2009 HC82”, for example, have the smallest and largest Δvs in the dataset (3.74 and 51.19 km/s respectively) but have estimated prices of $0.\n\n\n\n\n\n\n\n\n\n\nThe asteroids in this data subset are massive. In 2012 scientists from NASA and a variety of educational institutions wrote a report on the feasibility of asteroid retrieval. They came up with a plan using current technology and technology in development to bring a 7-meter, 500 ton asteroid into lunar orbit. They estimated the cost at more than 2 billion dollars. In the plot above a red line is placed at 7 meters. None of the asteroids in the subset of data we are working with that have defined diameters fall below this line. It is not currently feasible to bring any of them to the moon.\n\n\n\n\n\n\n\n\n\nThis paper did identify several potential candidates for capture and return. And, for NASA, the real profitability is in science and having access to materials in space. 500 tons of asteroid are 500 tons of material that do not need to be launched into space with expensive rockets.\n\n\n\n\n\nAsterank still sees these asteroids as quite profitable, if not currently feasible. The graph of estimated profit is strikingly similar to the graph of price. Even considering the higher cost of getting to the main belt it is still worthwhile. Expenses are considerable though, while the profit plot has a similar shape it is shifted down considerably. For the median asteroid in our data, expenses eat up 92% of the asteroids value.\nAll of this is hypothetical. There is not country or company that has or is about to capture, return, and mine an asteroid. And certainly not any of the large, high value asteroids in the main belt. Estimates for price and composition are just estimates and more research is needed to clarify them. Right now the focus is on gathering information on asteroids and Asterank is a great tool for generating interest and support for that research.\nNotably, NASA’s OSIRIS-REx mission is on track to return an asteroid sample to Earth in 2023. This, like meteorite composition studies, can be used to better link spectral and composition data."
  },
  {
    "objectID": "projects/asteroids/index.html#resources",
    "href": "projects/asteroids/index.html#resources",
    "title": "Asteroid Value Analysis",
    "section": "Resources",
    "text": "Resources\nAsterank\nAsteroids - Wikipedia\nBasic Asteroid Info - NASA\nObject Classifications - Planetary Data System\nSpectral Types - Wikipedia\nAsteroid Retrieval Feasibility Study\nOSIRIS-REx"
  },
  {
    "objectID": "projects/asteroids/index.html#appendix",
    "href": "projects/asteroids/index.html#appendix",
    "title": "Asteroid Value Analysis",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "projects/doublerobust/index.html",
    "href": "projects/doublerobust/index.html",
    "title": "Doubly Robust Estimators",
    "section": "",
    "text": "Doubly Robust estimation allows researchers to take advantage of both an outcome model and a model of treatment. If one or both of the models is correct, the doubly robust estimator will produce an accurate estimate of the causal effect. Here I’ll focus on doubly robust estimation for a quantitative outcome and a binary treatment. Here is one such estimator:\n\\[\\begin{aligned}\n&\\frac{1}{n}\\sum \\left[ \\hat{y}_1(\\textrm{covariates}_i) + \\frac{Y_i A_i}{\\hat{e}(\\textrm{covariates}_i)} - \\frac{\\hat{y}_1(\\textrm{covariates}_i)*A_i}{\\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n-&\\frac{1}{n}\\sum \\left[ \\hat{y}_0(\\textrm{covariates}_i) + \\frac{Y_i(1 - A_i) }{1 - \\hat{e}(\\textrm{covariates}_i)} - \\frac{\\hat{y}_0(\\textrm{covariates}_i)*(1 - A_i)}{1 - \\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{A}) \\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{B}) \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad (\\textrm{C})\n\\end{aligned}\\]\nOr, alternatively:\n\\[\\begin{aligned}\n(\\textrm{A})  \\hskip 2em &\\frac{1}{n}\\sum \\left[ \\hat{y}_1(\\textrm{covariates}_i) - \\hat{y}_0(\\textrm{covariates}_i) \\right]\\\\\n\\\\\n(\\textrm{B}) \\hskip 1.2em +&\\frac{1}{n}\\sum \\left[ \\frac{Y_i A_i}{\\hat{e}(\\textrm{covariates}_i)} - \\frac{Y_i(1 - A_i) }{1 - \\hat{e}(\\textrm{covariates}_i)} \\right]\\\\\n\\\\\n(\\textrm{C}) \\hskip 1.2em -&\\frac{1}{n}\\sum \\left[\\frac{\\hat{y}_1(\\textrm{covariates}_i)*A_i}{\\hat{e}(\\textrm{covariates}_i)} -  \\frac{\\hat{y}_0(\\textrm{covariates}_i)*(1 - A_i)}{1 - \\hat{e}(\\textrm{covariates}_i)} \\right] \\\\\n\\\\\n\\end{aligned}\\]\nWhere\n\n\\(\\hat{y}_1(\\textrm{covariates}_i)\\) are the predicted values for each individual \\(i\\) with treatment \\(1\\).\n\\(\\hat{y}_0(\\textrm{covariates}_i)\\) are the predicted values for each individual \\(i\\) with treatment \\(0\\).\n\\(Y_i\\) and \\(A_i\\) are, respectively, the actual outcome and treatment values for each individual.\n\\(\\hat{e}(\\textrm{covariates}_i)\\) are the predicted propensity scores for each individual \\(i\\).\n\nThe rest of this page will attempt to explain how the doubly robust property works in this estimator."
  },
  {
    "objectID": "projects/doublerobust/index.html#sources",
    "href": "projects/doublerobust/index.html#sources",
    "title": "Doubly Robust Estimators",
    "section": "Sources",
    "text": "Sources\n\n“Doubly Robust Estimation of Causal Effects” by Funk et al. and “A Falsifiability Characterization of Double Robustness Through Logical Operators” by Frangakis were used heavily in this project. My goal in this project was essentially to create an explanation of double robustness that combined Frangakis’ logical framework with the more numerical and simulation-based approach of the Funk paper.\nDoubly Robust Estimation of Causal Effects\nA Falsifiability Characterization of Double Robustness Through Logical Operators"
  },
  {
    "objectID": "projects/doublerobust/index.html#code",
    "href": "projects/doublerobust/index.html#code",
    "title": "Doubly Robust Estimators",
    "section": "Code",
    "text": "Code\n\n\nSimulating Data\n\nset.seed(7)\nn &lt;- 100000\nZ &lt;- rnorm(n, 1, 2) # Z\n\nW &lt;- rbinom(n, 1, .5) # W\n\nX &lt;- rnorm(n, 2*W, 2) # X\n\nM &lt;- rnorm(n, 20, 1) # M\n\nN &lt;- rnorm(n, 10, 2) # N\n\nlog_odds_A &lt;- .25 * Z + .5 * X + .1 * N\nodds_A &lt;- exp(log_odds_A)\np_A &lt;- odds_A / (1 + odds_A)\nA &lt;- rbinom(n, 1, p_A) # A\n\n\n\nY &lt;- rnorm(n, A + W + 2 * Z + 0.05 * M + rnorm(n, 2, 1), .25) # Y\n\ndata &lt;- data.frame(A = (A), Y = Y, Z = Z, M = M, W = (W), X = X)\n\n\n\nFunction for Normal Methods\n\nget_normal_methods &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n  \n  \n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n  \n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n  # print(summary(example_ipw)$coefficients)\n  # print(summary(outcome)$coefficients)\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2]))\n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\",\"Causal Effect Estimate from Outcome Model\", \"Causal Effect Estimate from Treatment Model\")\n  return(output)\n}\n\n\n\nFunction for Two Parts of DR Estimator\n\nget_dr_parts2_3 &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n  \n  forceA0 &lt;- data %&gt;% mutate(A = 0)\n  forceA1 &lt;- data %&gt;% mutate(A = 1)\n\n  predA0 &lt;- predict(outcome, newdata = forceA0)\n  predA1 &lt;- predict(outcome, newdata = forceA1)\n\n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n  part1 &lt;- predA1\n  part2 &lt;- ((data$Y * data$A) / (data$ps))\n  part3 &lt;- -((predA1 * data$A) / data$ps)\n\n\n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2], mean(part2), mean(-part3), mean(part2+part3)))\n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\", \"Outcome Model Estimate\", \"IPW Estimate\", \"$B$\", \"$C$\", \"D\")\n  return(output)\n}\n\n\n\nFunction for All Parts of DR Estimator\n\nget_dr_parts &lt;- function(data, outcome_formula, exposure_formula) {\n  outcome &lt;- lm(outcome_formula)\n  ps_model &lt;- glm(exposure_formula, family = \"binomial\")\n\n\n  data &lt;- data %&gt;%\n  mutate(ps = predict(ps_model, newdata = data, type = \"response\")) %&gt;%\n  mutate(ipw = case_when(\n            A == 1 ~ 1/ps,\n            A == 0 ~ 1/(1-ps)\n        ))\n\n  example_ipw &lt;- lm(Y ~ A, data = data, weights = ipw)\n\n  forceA0 &lt;- data %&gt;% mutate(A = 0)\n  forceA1 &lt;- data %&gt;% mutate(A = 1)\n\n  predA0 &lt;- predict(outcome, newdata = forceA0)\n  predA1 &lt;- predict(outcome, newdata = forceA1)\n\n  DR1 &lt;-\n    (((    data$A) * data$Y) / (    data$ps)) -      # IPW part\n    ((predA1 * (data$A - data$ps))) / (    data$ps)  # Other thing\n  DR0 &lt;-\n    (((1 - data$A) * data$Y) / (1 - data$ps)) +      # IPW part\n    ((predA0 * (data$A - data$ps))) / (1 - data$ps)  # Other thing\n\n  Estimator1 &lt;- predA1 + ((data$Y * data$A) / (data$ps)) - ((predA1 * data$A) / data$ps)\n  Estimator0 &lt;- predA0 + ((data$Y * (1 - data$A)) / (1 - data$ps)) - ((predA0 * (1 - data$A)) / (1 - data$ps))\n\n\n  part1 &lt;- predA1\n  part2 &lt;- ((data$Y * data$A) / (data$ps))\n  part3 &lt;- -((predA1 * data$A) / data$ps)\n  \n  part1A0 &lt;- predA0\n  part2A0 &lt;- ((data$Y * (1 - data$A)) / (1 - data$ps))\n  part3A0 &lt;- -((predA0 * (1 - data$A)) / (1 - data$ps))\n  \n  partdr1 &lt;- mean(part1) - mean(part1A0)\n  partdr2 &lt;- mean((data$Y * data$A) / (data$ps)) - mean((data$Y * (1 - data$A)) / (1 - data$ps))\n  partdr3 &lt;- mean(-((predA1 * data$A) / data$ps)) - mean(-((predA0 * (1 - data$A)) / (1 - data$ps)))\n\n  mods &lt;- t(c(outcome_formula,exposure_formula))\n  output &lt;- as.data.frame(cbind(mods, outcome$coefficients[2], example_ipw$coefficients[2], partdr1, partdr2, partdr3, sum(partdr1, partdr2, partdr3)))\n  \n  names(output) &lt;- c(\"Outcome Formula\",\"Exposure Formula\", \"Outcome Model Estimate\", \"IPW Estimate\", \"$B$\", \"$C$\", \"D\", \"E\")\n  return(output)\n}"
  },
  {
    "objectID": "projects/pagerank/index.html",
    "href": "projects/pagerank/index.html",
    "title": "PageRank",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\nPageRank with Eigenvectors cla365 technical report\n\n\n\n \n\n\n\n\n\nPageRank with Eigenvectors cla365 technical report \n\n\n Nathaniel Reimer     Preston Locke \n\n\n(April 30, 2022)\n\n\n\nAbstract\n\n\nIn this report we examine the PageRank algorithm by showing that the Google Matrix is stochastic. We then present R code that implements the PageRank algorithm and demonstrate it on a simple network. As an extension, we explore some permutations on this network as well as the implications of changing the random jump probability (q)\n\n\n\n\n1 Implementing PageRank\n\n\n\n1.1 Proving the Google Matrix is Stochastic\n\n\n\nProving the Google matrix G is stochastic requires showing that G is nonnegative and that each of its columns sum to 1. We’ll start by proving that the matrix is nonnegative. Note that for each entry in G, we have the following:\n\n\n\n\n\n\n\n\nGi⁢j=q/n+Aj⁢i⁢(1-q)/nj\n\n\n\n\n\n\n\n\nWe already know the following facts about the variables in the expression above:\n\n\n\n\n\n1.\n\n\n0≤q≤1\n\n\n\n\n2.\n\n\nn&gt;0\n\n\n\n\n3.\n\n\nAj⁢i∈{0,1}\n\n\n\n\n4.\n\n\nnj=∑i=1nAj⁢i\n\n\n\n\n\n\n\nFrom facts 1, 2, and 3, we also know that q/n, Aj⁢i, and (1-q) are all nonnegative numbers. And then, by facts 3 and 4, nj must also be nonnegative. Therefore, we can say that the entire expression is nonnegative, and thus, each entry in G is nonnegative by definition.\n\n\n\n\nNext, we show that each column in G sums to 1, or in other words, that for each j∈{1,2,…,n} we show that:\n\n\n\n\n\n\n\n\n∑i=1nq/n+Aj⁢i⁢(1-q)/nj=1\n\n\n\n\n\n\n\n\nThe first term in the summation can be pulled out because it’s easy to see that q/n added n times will just equal q. We can also pull out the expression ((1-q)/nj) from the summation, because everything in it is a constant with respect to i. Doing these two things gives us the below:\n\n\n\n\n\n\n\n\nq+((1-q)/nj)⁢∑i=1nAj⁢i\n\n\n\n\n\n\n\n\nNow, using fact 4 from the first part of our proof, we can simplify ∑i=1nAj⁢i to nj and finish simplifying:\n\n\n\n\n\n\n\n\nq+((1-q)/nj)⁢nj\n\n\n\n\n\n\n\n\n\n\nq+(1-q)\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\nWith that, we have successfully shown that G is a nonnegative matrix whose columns sum to 1, so we have proven that G is stochastic.\n\n\n\n\n\n1.2 Calculating the Google Matrix and Page Ranks\n\n\n\nTo calculate page ranks for a given network of pages, we can use the fact that G is stochastic and find its dominant eigenvector, which will give us the rankings. If we scale that eigenvector so that its entries sum to 1, we’ll have our answer. To do this, we’ve written two R functions, GoogleMatrix and PageRankVector, which take in an n×n adjacency matrix adj and output, respectively, the n×n Google matrix of the network and a vector of page ranks given the value of q (which defaults to 0.15).\n\n\n\nGoogleMatrix = function(adj, q=0.15) {\n  n = dim(adj)[1]\n  G = matrix(nrow=n, ncol=n)\n  for (j in 1:n) {\n    # Get the sum of row j\n    n.j = sum(adj[j,])\n    for (i in 1:n) {\n      # Calculate the ijth entry of G\n      G[i,j] = q/n + adj[j,i] * (1 - q) / n.j\n    }\n  }\n  return(G)\n}\nPageRankVector = function(adj, q=0.15) {\n  G = GoogleMatrix(adj, q=q)\n  # Find the dominant eigenvector of G\n  p = eigen(G)$vectors[,1]\n  # Return the normalized version (inf. norm)\n  return(p / sum(p))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Network of 15 pages, taken from page 550 of the textbook.\n\n\n\n\nWe can test these functions on the network in Figure 1 that is taken from the textbook. The result of running GoogleMatrix is shown in Appendix A, but the result of running PageRankVector is shown below:\n\n\n\n&gt; A = rbind(\n+     c(0,1,0,0,0,0,0,0,1,0,0,0,0,0,0),\n+     c(0,0,1,0,1,0,1,0,0,0,0,0,0,0,0),\n+     c(0,1,0,0,0,1,0,1,0,0,0,0,0,0,0),\n+     c(0,0,1,0,0,0,0,0,0,0,0,1,0,0,0),\n+     c(1,0,0,0,0,0,0,0,0,1,0,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,0,0,0),\n+     c(0,0,0,1,0,0,0,0,0,0,1,0,0,0,0),\n+     c(0,0,0,0,1,1,0,0,0,1,0,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0),\n+     c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1),\n+     c(0,0,0,0,0,0,1,1,0,0,1,0,0,0,0),\n+     c(0,0,0,0,0,0,0,0,1,0,0,0,0,1,0),\n+     c(0,0,0,0,0,0,0,0,0,1,1,0,1,0,1),\n+     c(0,0,0,0,0,0,0,0,0,0,0,1,0,1,0)\n+ )\n&gt; PageRankVector(A)\n [1] 0.027 0.030 0.030 0.027 0.040 0.040 0.040 0.040 0.075 0.106 0.106 0.075 0.125 0.116 0.125\n\n\n\n\nHigher numbers in the PageRank vector correspond to higher rankings. So, using the PageRank method with q=.15 makes 13 and 15 the highest ranked websites and 1 and 4 the lowest ranked.\n\n\n\n\n\n\n2 Further Exploration\n\n\n\n2.1 Other Values of q\n\n\n\nThe jump probability, q, is the probability that a web-surfer chooses any random website rather than clicking one of the links on their current website. We can illustrate this using the equation for the entries to the Google matrix G.\n\n\n\n\n\n\nGi⁢j=qn+Ai⁢j⁢(1-q)/nj\n\n\n\n\n\n\nIf q=1 then Gi⁢j=qn since the other term is multiplied by 0. As we expect, the probability of ending up at any particular website is just 1/n where n is the number of websites: after visiting a website a web-surfer simply chooses another website at random from the list. We use the image() function in R to visualize the page rank vector for different values of q. In Figure 2, higher values in the PageRank vector are in yellow and lower values are in dark blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Heatmap of PageRank vectors at different q values.\n\n\n\n\nThe result we obtain from Figure 2 is precisely what we expect. At a q value of 1 all of the websites’ entries in the PageRank vector are exactly equal (they are the same color). Moreover, as we decrease q differences appear and become more pronounced. When q is zero and the probability of ending up at any given website is based solely on the network structure: There are no random jumps. For PageRank, Google used a q value of .15 so that network structure is the primary determining factor but some random jumping occurs. This choice has a number of interesting implications for our network. We first observed that while 15, 14, and 13 have similar values when q is zero, when q reaches .15 website 14 has fallen further than 13 and 15. Additionally, 2 and 3 have lower values when q is zero but higher values when q is .15. It would be unrealistic to assume there is no random jumping when someone is surfing the web and not accounting for it leads to an incorrect page rank, especially in the case of 1, 2, 3, and 4. Why, exactly, these changes happen in this network is certainly worthy of further study. It may come about because of the symmetric connections between 2 and 3 and those between 13, 14, and 15.\n\n\n\n\n\n2.2 Removing a Node\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Network with Page 10 removed.\n\n\n\n\nWhat happens to the ranks of the other pages if we remove a page from the network? For a start, we can predict that the effect on the other page ranks will be more pronounced when a page with a higher rank is removed. Being a probability distribution, the page rank vector needs to add to 1, and thus the value of the removed page’s rank will need to be redistributed to the other pages somehow, in order to maintain the sum of 1. The higher the removed page’s rank, the more value will be redistributed to the other pages.\n\n\n\n\nExploring this theme of removing highly-ranked pages further, let’s look at a concrete example using the 15-page example from before. This time, we’ll remove page 10 to get the network shown in Figure 3. Below are the page ranks we calculated for the modified network:\n\n\n\n&gt; PageRankVector(A[c(1:9, 11:15), c(1:9, 11:15)])\n [1] 0.047 0.041 0.036 0.032 0.043 0.041 0.052 0.050 0.048 0.171 0.104 0.041 0.107 0.186\n\n\n\n\nComparing these rankings to those of the 15-page network, we see that almost all of the pages rose in rank, except for pages 9, 13, and 14. Note that page 13 was pointed to by page 10, and page 13 points to pages 9 and 14. It seems that the popularity of page 10 contributed to propping up the rankings of pages 9, 13, and 14, so when it was removed, those page rankings fell.\n\n\n\n\nIt should be noted, however, that in the given network, there is a directed path from page 10 to every other page in the network, meaning that to some degree, all pages are pointed to by page 10, implying that page 10’s popularity should have had some effect of propping those pages up, if only small. We believe the reason why the rankings of the other pages did not fall is a combination of their degree of separation from page 10 (longer paths yield a smaller influence on ranking) and other factors (like a change in a page’s ratio of in-links to out-links).\n\n\n\n\n\n2.3 Rank Strategy\n\n\n\nWe then considered a possible strategy for a website to increase its PageRank. Specifically, what happens when websites 2 and 12 favor linking to website 7. We model this by changing the corresponding entries in the adjacency matrix from 1 to 2. In Figure 4 we show this change by bolding these two links. We then calculated the PageRank vector p2 for the modified matrix and calculated a ratio of it and the original PageRank vector p1 to obtain a vector p2p1.\n\n\n\n\n\n\n\n\n(p2/p1)T=\n\n\n\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]  [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] 0.969 0.954 0.878 0.892 0.951 0.986 1.33 0.829 1.02  1.05 0.971  0.97  1.04  1.01 0.981\n\n\n\n\n\nFigure 4: Modified network with more prominent links to website 7.\n\n\n\n\nUnsurprisingly, strategy has benefited page 7’s value in the PageRank vector. We decided to take a closer look at how the strategy impacted the values of other sites. We immediately noticed that while 11 and 10 are both linked to by page 7, only page 10’s value increased. This can be explained by examining the network structure. In the google matrix for the modified network the probability of moving from 12 to 7 is increased, this means that the probability of moving from 12 to 11 or 4 is decreased. So even though 11 may benefit from being linked to by 7 this is out weighed by the negative effect of being deprioritized by 12. 10 does not face this negative effect and thus benefits.\n\n\n\n\n\n\n\n\nAppendix A Appendix: Unmodified 15-Page Google Matrix\n\n\n&gt; GoogleMatrix(A)\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n [1,] 0.01 0.01 0.01 0.01 0.43 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [2,] 0.43 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [3,] 0.01 0.29 0.01 0.43 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [4,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.43 0.01  0.01  0.01  0.01  0.01  0.01  0.01\n [5,] 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01 0.29  0.01  0.01  0.01  0.01  0.01  0.01\n [6,] 0.01 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.29  0.01  0.01  0.01  0.01  0.01  0.01\n [7,] 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.29  0.01  0.01  0.01\n [8,] 0.01 0.01 0.29 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.29  0.01  0.01  0.01\n [9,] 0.43 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.43  0.01  0.01\n[10,] 0.01 0.01 0.01 0.01 0.43 0.43 0.43 0.01 0.29  0.01  0.01  0.01  0.01  0.22  0.01\n[11,] 0.01 0.01 0.01 0.01 0.01 0.43 0.43 0.43 0.01  0.01  0.01  0.29  0.01  0.22  0.01\n[12,] 0.01 0.01 0.01 0.43 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.01  0.01  0.43\n[13,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.86  0.01  0.01  0.01  0.22  0.01\n[14,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.01  0.01  0.43  0.01  0.43\n[15,] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01  0.01  0.86  0.01  0.01  0.22  0.01\n\n\n\n\n\n\n\nGenerated by LaTeXML"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPageRank\n\n\n\nData Science\n\n\nLinear Algebra\n\n\n\nImplementing and Exploring a Simplified Version of the PageRank Algorithm\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDoubly Robust Estimators\n\n\n\nData Science\n\n\nStatistics\n\n\n\nAn Exploration of a Double Robust Estimator for Causal Inference\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsteroid Value Analysis\n\n\n\nData Science\n\n\nSpace\n\n\n\nVisualization and Analysis of Asterank data\n\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]